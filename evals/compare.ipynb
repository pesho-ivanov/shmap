{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "#from astropy import units as u\n",
    "import sys\n",
    "from readpaf import parse_paf\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sv\n",
    "\n",
    "#from itables import init_notebook_mode\n",
    "#init_notebook_mode(all_interactive=True)\n",
    "\n",
    "def to_latex(df, data, refname):\n",
    "    latex = \"\"\n",
    "    df.index = df.index.map(lambda x: f'\\\\{x}')\n",
    "    df.columns = df.columns.str.replace(' ', '\\\\\\\\')\n",
    "    df.columns = df.columns.str.replace('%', '\\%')\n",
    "    df.columns = df.columns.map(lambda x: '\\makecell{' + x + '}')\n",
    "    #df = df.astype(str).map(lambda x: x.rstrip('0').rstrip('.') if '.' in x else x)\n",
    "    latex += df.to_latex(escape=False, label=f'tab:{refname}', caption=data, float_format = lambda x: '{:0.2f}'.format(x) if pd.notna(x) else '-')\n",
    "    #latex += df.to_latex(float_format = lambda x: '{:0.2f}'.format(x) if pd.notna(x) else '-')\n",
    "    latex += '\\n'\n",
    "    return latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_length(df):\n",
    "    df_sorted = df.sort_values(by='target_start').reset_index(drop=True)\n",
    "    merged_intervals = []\n",
    "    current_start, current_end = df_sorted.iloc[0]['target_start'], df_sorted.iloc[0]['target_end']\n",
    "\n",
    "    for index, row in df_sorted.iterrows():\n",
    "        if index == 0:\n",
    "            continue\n",
    "        if row['target_start'] <= current_end:\n",
    "            current_end = max(current_end, row['target_end'])\n",
    "        else:\n",
    "            merged_intervals.append((current_start, current_end))\n",
    "            current_start, current_end = row['target_start'], row['target_end']\n",
    "\n",
    "    merged_intervals.append((current_start, current_end))\n",
    "    union_length = sum(end - start for start, end in merged_intervals)\n",
    "    return union_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perc(a, b):\n",
    "    if b == 0:\n",
    "        return np.nan\n",
    "    return 100.0 * a / b\n",
    "\n",
    "def fasta2df(fn):\n",
    "    seqs = SeqIO.parse(fn, \"fasta\")\n",
    "    df = pd.DataFrame((str(s.id), str(s.seq)) for s in seqs)\n",
    "    df.columns = [\"ID\", \"Sequence\"]\n",
    "    return df\n",
    "\n",
    "def is_overlapping(a, sv_row):\n",
    "    return a.GT_from <= sv_row['END'] and sv_row['POS'] <= a.GT_to \n",
    "    \n",
    "min_overlap = 0.9\n",
    "\n",
    "# def is_correct(a):\n",
    "#     if a.GT_ref != a.target_name:\n",
    "#         return False\n",
    "#     if a.GT_strand != a.strand:\n",
    "#         return False\n",
    "#     union_from = min(a.GT_from, a.target_start)\n",
    "#     union_to = max(a.GT_to, a.target_end)\n",
    "\n",
    "#     intersect_from = max(a.GT_from, a.target_start)\n",
    "#     intersect_to = min(a.GT_to, a.target_end)\n",
    "#     overlaps = intersect_to - intersect_from >= min_overlap * (union_to - union_from)\n",
    "#     return overlaps\n",
    "\n",
    "def is_correct_labels(a, GT_l, target_l, debug=False):\n",
    "    if a.GT_ref != a.target_name:\n",
    "        return False\n",
    "    if a.GT_strand != a.strand:  # won't work for inversions\n",
    "        return False\n",
    "#    union_from = min(a.GT_from, a.target_start)\n",
    "#    union_to = max(a.GT_to, a.target_end)\n",
    "\n",
    "    GT_labels = Counter(target_l[a.GT_ref][a.GT_from:a.GT_to])\n",
    "    target_labels = Counter(GT_l[a.target_name][a.target_start:a.target_end])\n",
    "    #GT_labels = Counter(GT_l[a.GT_ref][a.GT_from:a.GT_to])\n",
    "    #target_labels = Counter(target_l[a.target_name][a.target_start:a.target_end])\n",
    "    intersection = GT_labels & target_labels\n",
    "    union = GT_labels | target_labels\n",
    "    overlaps = sum(intersection.values()) >= min_overlap * sum(union.values())\n",
    "\n",
    "    if debug:\n",
    "        display(a)\n",
    "        display('           GT from {} to {}'.format(a.GT_from, a.GT_to))\n",
    "        display('       target from {} to {}'.format(a.target_start, a.target_end))\n",
    "        display('    GT_labels from {} to {}'.format(min(GT_labels), max(GT_labels)))\n",
    "        display('target_labels from {} to {}'.format(min(target_labels), max(target_labels)))\n",
    "        display('{} >?= {} = {} * union {} => {}'.format(sum(intersection.values()), min_overlap * sum(union.values()), min_overlap, sum(union.values()), overlaps))\n",
    "    return overlaps\n",
    "\n",
    "def is_correct_labels_df(df: pd.DataFrame, GT_l, target_l, debug=False):\n",
    "    if (df.GT_ref != df.target_name).any():\n",
    "        return False\n",
    "    if (df.GT_strand != df.strand).any():  # won't work for inversions\n",
    "        return False\n",
    "\n",
    "    GT_ref, GT_from, GT_to = df.GT_ref.iloc[0], df.GT_from.iloc[0], df.GT_to.iloc[0]\n",
    "    assert (df.GT_ref == GT_ref).all() and (df.GT_from == GT_from).all() and (df.GT_to == GT_to).all()\n",
    "\n",
    "    GT_labels = Counter(target_l[GT_ref][GT_from:GT_to])\n",
    "    L = [ Counter(GT_l[a.target_name][a.target_start:a.target_end]) for a in df.itertuples() ]\n",
    "    target_labels = sum(L, Counter())\n",
    "    #GT_labels = Counter(GT_l[a.GT_ref][a.GT_from:a.GT_to])\n",
    "    #target_labels = Counter(target_l[a.target_name][a.target_start:a.target_end])\n",
    "    intersection = GT_labels & target_labels\n",
    "    union = GT_labels | target_labels\n",
    "    overlaps = sum(intersection.values()) >= min_overlap * sum(union.values())\n",
    "    return overlaps\n",
    "\n",
    "def read_falls_on_what_sv(a, vcf_df):\n",
    "    query_start = a['GT_from']\n",
    "    query_end = a['GT_to']\n",
    "    start_idx = vcf_df['POS'].searchsorted(query_start, side='right')\n",
    "    end_idx = vcf_df['END'].searchsorted(query_end, side='left')\n",
    "    overlap_df = vcf_df.iloc[start_idx:end_idx]\n",
    "    overlap_ch_df = overlap_df[overlap_df['CHROM'] == a['GT_ref']]\n",
    "    overlap_ch_sv = overlap_ch_df[(overlap_ch_df['POS'] <= query_end) & (overlap_ch_df['END'] >= query_start)]\n",
    "    #if len(overlap_ch_sv) > 0:\n",
    "    #    display(a, overlap_ch_sv)\n",
    "    if len(overlap_ch_sv) == 0:\n",
    "        return 'none'\n",
    "    elif len(overlap_ch_sv) == 1:\n",
    "        return overlap_ch_sv.iloc[0]['SVTYPE']\n",
    "    else:\n",
    "        return 'multi'\n",
    "\n",
    "def read_paf(pref, reads, experiment, tool, orig_l: dict, mutated_l: dict, vcf_df: pd.DataFrame):\n",
    "    #display(vcf_df)\n",
    "    paf_file = pref.with_suffix('.paf')\n",
    "    no_GT = False\n",
    "    if not paf_file.exists():\n",
    "        raise Exception(f\"File does not exist or is empty: {paf_file}\")\n",
    "    with open(paf_file) as handle:\n",
    "        df = parse_paf(handle, dataframe=True)\n",
    "        df['experiment'] = experiment\n",
    "        df['tool'] = tool\n",
    "        try:\n",
    "            df[ ['read_name', 'GT_ref', 'GT_from', 'GT_to', 'GT_strand'] ] = df['query_name'].str.split('!', expand=True)\n",
    "            df['GT_from'] = df['GT_from'].astype(int)\n",
    "            df['GT_to'] = df['GT_to'].astype(int)\n",
    "            #df['is_correct_labels'] = df.apply(lambda x: is_correct_labels(x, orig_l, mutated_l), axis=1)\n",
    "            #df['is_correct'] = df.apply(is_correct, axis=1)\n",
    "            df['is_correct'] = df.apply(lambda x: is_correct_labels(x, orig_l, mutated_l), axis=1)\n",
    "            df['start_diff'] = df.target_start - df.GT_from  # TODO: different coordinate systems!\n",
    "            df['end_diff'] = df.target_end - df.GT_to  # TODO: different coordinate systems!\n",
    "            df['read_sv'] = df.apply(lambda x: read_falls_on_what_sv(x, vcf_df), axis=1)\n",
    "        except Exception as e:\n",
    "            display(e)\n",
    "            df['read_name'] = df['query_name']\n",
    "            #df['is_correct_labels'] = True\n",
    "            df['is_correct'] = True\n",
    "            df['start_diff'] = 0\n",
    "            df['end_diff'] = 0\n",
    "            df['read_sv'] = 'none'\n",
    "            no_GT = True\n",
    "        #display(df)\n",
    "    df = df.sort_values(['read_name', 'residue_matches'], ascending=[True, False], ignore_index=True)\n",
    "\n",
    "    # initialize from left to right\n",
    "    myNA = 'N/A'\n",
    "    #bad = 'bad'\n",
    "    unique = 'uniq'\n",
    "    multi = 'mult'\n",
    "    #bad_mappings_perc = bad+' mappings p.multiread'\n",
    "    paf = defaultdict(int)\n",
    "    #paf['unmapped'] = 0\n",
    "    #paf[unique] = 0\n",
    "    #paf[multi] = 0\n",
    "    #for suffix in ['', ' Q60']:\n",
    "    #    paf[bad + suffix] = 0\n",
    "    #    #if suffix == '':\n",
    "    #        #paf[multi+' '+bad+' group'] = 0\n",
    "    #        #paf[bad_mappings_perc] = 0\n",
    "    #    for unique_or_multi in [unique, multi]:\n",
    "    #        paf[bad + ' ' + unique_or_multi + suffix] = 0\n",
    "\n",
    "    def bad(sv_type):\n",
    "        return 'bad ' + sv_type\n",
    "\n",
    "    def process_group(group_first_index, group_last_index):\n",
    "        group = df.loc[group_first_index:group_last_index]\n",
    "        first_in_group = df.loc[group_first_index]\n",
    "        #group_is_correct = group['is_correct'].any()\n",
    "        group_is_correct = is_correct_labels_df(group, orig_l, mutated_l)\n",
    "        group_is_unique = len(group) == 1\n",
    "        suffix = 'Q60' if (group.mapping_quality == 60).all() else ''\n",
    "\n",
    "        unique_or_multi = unique if group_is_unique else multi\n",
    "        paf[unique_or_multi] += 1\n",
    "        if not group_is_correct:\n",
    "            paf[bad(suffix)] += 1\n",
    "            paf[unique_or_multi + ' ' +bad(suffix)] += 1\n",
    "            #if not group_is_unique:\n",
    "            #    paf[multi+' '+bad+' group'] += 1\n",
    "            #    paf[bad_mappings_perc] += 1\n",
    "            paf[bad(first_in_group['read_sv'])] += 1\n",
    "        paf[first_in_group['read_sv']] += 1\n",
    "\n",
    "    group_first_i, group_read_name = 0, df.loc[0, 'read_name']\n",
    "    for i, a in df.iterrows():\n",
    "        if a.read_name != group_read_name:\n",
    "            process_group(group_first_i, i-1)\n",
    "            group_first_i, group_read_name = i, a.read_name\n",
    "    process_group(group_first_i, len(df)-1)\n",
    "\n",
    "    paf['unmapped'] = reads - paf[unique] - paf[multi]\n",
    "\n",
    "    paf['aligned_with 0/1/2+_segments'] = '{} / {} / {} ({:.0f}%/{:.0f}%/{:.0f}%)'.format(\n",
    "        paf['unmapped'], paf[unique], paf[multi],\n",
    "        perc(paf['unmapped'], len(df)), perc(paf[unique], len(df)), perc(paf[multi], len(df)))\n",
    "    #for key in [unique, multi, 'unmapped', bad]:\n",
    "    #    paf[key] = '{} ({:.2f}%)'.format(paf[key], perc(paf[key], reads))\n",
    "\n",
    "    #if paf[multi] > 0:\n",
    "    #    paf[bad_mappings_perc] = '{:.2f}'.format(paf[bad_mappings_perc] / paf[multi])\n",
    "    #else:\n",
    "    #    paf[bad_mappings_perc] = myNA\n",
    "\n",
    "    paf['mean start diff'] = '{:.1f}'.format(df[df.is_correct].start_diff.mean())\n",
    "    paf['mean end diff'] = '{:.1f}'.format(df[df.is_correct].end_diff.mean())\n",
    " \n",
    "    #if no_GT:\n",
    "    #    for suff in ['', ' '+unique, ' '+multi]:\n",
    "    #        for suffsuff in ['', ' Q60']:\n",
    "    #            paf[bad + suff + suffsuff] = myNA \n",
    "    #        #paf[multi+' '+bad+' group'] = myNA\n",
    "    #        paf['addit '+bad+' aligns'] = myNA\n",
    "    #    paf['mean start diff'] = myNA\n",
    "    #    paf['mean end diff'] = myNA\n",
    "\n",
    "    paf['alignments depth'] = '{:.2f}'.format(sum(df['query_length']) / df.iloc[0]['target_length'])\n",
    "    union_len = union_length(df)\n",
    "    paf['covered reference'] = '{} ({:.2f}%)'.format(union_len, perc(union_len, df.iloc[0]['target_length']))\n",
    "\n",
    "    print(vcf_df)\n",
    "    for sv_type in list(vcf_df['SVTYPE'].unique()) + ['none', 'multi']:\n",
    "        paf[bad(sv_type)] = '{} / {} ({:.2f}%)'.format(paf[bad(sv_type)], paf[sv_type], perc(paf[bad(sv_type)], paf[sv_type]))\n",
    "        paf.pop(sv_type)\n",
    "\n",
    "    # debug\n",
    "    #for row in df[df['read_sv'] != 'none'].itertuples():\n",
    "    #    is_correct_labels(row, orig_l, mutated_l, debug=True)\n",
    "\n",
    "    paf.pop('unmapped')\n",
    "    paf.pop(unique)\n",
    "    paf.pop(multi)\n",
    "\n",
    "    return pd.Series(paf, dtype='object')\n",
    "    \n",
    "def read_times(pref):\n",
    "    times = {}\n",
    "    with open(str(pref) + '.index.time') as f_index_time:\n",
    "        index_time, index_mem = map(float, f_index_time.readline().split())\n",
    "        times['index time'] = index_time #* u.second\n",
    "        #times['index_mem'] = index_mem / 2**20\n",
    "        with open(str(pref) + '.time') as f_time:\n",
    "            total_time, total_mem = map(float, f_time.readline().split())\n",
    "            #times['time total'] = total_time #* u.second\n",
    "            times['map time'] = total_time - times['index time']\n",
    "            times['memory'] = (total_mem / 2**20) #* u.GB\n",
    "    return pd.Series(times, dtype='object').map('{:.1f}'.format)\n",
    "\n",
    "def get_comparison_table(refname, readsim_refname, experiment, tools):\n",
    "    empty_cell = -1\n",
    "    alldf = pd.DataFrame()\n",
    "    alldf.name = experiment\n",
    "    ref = fasta2df(Path('refs') / (refname+'.fa'))\n",
    "    reads = fasta2df(Path('reads') / Path(experiment+'.fa'))\n",
    "\n",
    "    # SVs\n",
    "    orig_fa_dict = sv.read_fasta_file(Path('refs') / (refname+'.fa'))\n",
    "    orig_l = sv.gen_unique_labels(orig_fa_dict)\n",
    "    try:\n",
    "        vcf_df = sv.read_vcf(Path('refs') / (readsim_refname+'.vcf'))\n",
    "        assert((vcf_df.POS <= vcf_df.END).all())\n",
    "        vcf_df = vcf_df.sort_values(by='POS').reset_index(drop=True)\n",
    "        mutated_fa, mutated_l = sv.mutate(orig_fa_dict, orig_l, vcf_df)\n",
    "    except FileNotFoundError:\n",
    "        vcf_df = sv.read_vcf(Path('refs') / 'empty.vcf')\n",
    "        mutated_fa, mutated_l = orig_fa_dict, orig_l\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for tool in tqdm(tools, desc=f'Tools for {experiment}'):\n",
    "        d = Path(\"out\") / experiment / tool / tool\n",
    "        row = pd.Series({\n",
    "            'tool': tool,\n",
    "            'reads': len(reads),\n",
    "        })\n",
    "        row = pd.concat([row, read_paf(d, len(reads), experiment, tool, orig_l, mutated_l, vcf_df)])     # .paf\n",
    "        try:\n",
    "            row = pd.concat([row, read_times(d)])   # .time, .index.time\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while reading times {d}: {e}\")\n",
    "            row['index time'] = empty_cell\n",
    "            row['map time'] = empty_cell\n",
    "            row['memory'] = empty_cell\n",
    "        rows.append(row)\n",
    "    alldf = pd.DataFrame(rows)\n",
    "    alldf = alldf.set_index('tool')\n",
    "    alldf.index.name = None\n",
    "    return alldf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a table to compare the mappers by accuracy, runtime (indexing and mapping) and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       " <style> table { font-family: \"Courier New\", Courier, monospace; } </style> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tools for t2tChrY-readst2tChrY-a0.99-d10-l10000:  33%|███▎      | 1/3 [15:03<30:07, 903.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         CHROM          POS              ID REF  ALT  QUAL FILTER  PRECISE SVTYPE      SVMETHOD  \\\n",
      "0  NC_060948.1  99999999999  [DUP0SURVIVOR]   N  DUP  None   PASS     True    DUP  SURVIVOR_sim   \n",
      "\n",
      "          CHR2          END  SVLEN dup_num  \n",
      "0  NC_060948.1  99999999999    616       2  \n"
     ]
    }
   ],
   "source": [
    "#tools = ['sweepmap', 'sweepmap-slow', 'mapquik', 'blend', 'minimap', 'winnowmap', 'rmqmap'] \n",
    "#tools = ['sweepmap', 'minimap', 'winnowmap', 'rmqmap']\n",
    "tools = ['sweepmap', 'sweepmap-slow', 'rmqmap']\n",
    "experiments = [\n",
    "    ('t2tChrY', 't2tChrY', 't2tChrY-readst2tChrY-a0.99-d10-l10000'),\n",
    "#    ('chm13',   'chm13',   'chm13-readschm13-a0.99-d0.1-l10000'),  # CRASH\n",
    "    ('t2tChrY', 't2tChrY', 't2tChrY-readst2tChrY-a0.99-d1-l24000'),\n",
    "#    ('chm13',   'chm13',   'HG002_24kb'),\n",
    "#\t('t2tChrY', 't2tChrY-SVs', 't2tChrY-readst2tChrY-SVs-a0.99-d0.1-l10000'),\n",
    "]\n",
    "\n",
    "pd.set_option('display.width', 100)\n",
    "css = \"\"\" <style> table { font-family: \"Courier New\", Courier, monospace; } </style> \"\"\"\n",
    "display(HTML(css))\n",
    "dfs = []\n",
    "keys = []\n",
    "for refname, readsim_refname, data in experiments:\n",
    "    vcf_df = get_comparison_table(refname=refname, readsim_refname=readsim_refname, experiment=data, tools=tools).round(2)\n",
    "    dfs.append(vcf_df)\n",
    "    keys.append(data)\n",
    "    df_styled = vcf_df.style.set_caption(data)\n",
    "    display(df_styled)\n",
    "    #print(vcf_df.to_latex(escape=True))\n",
    "#DF = pd.concat(dfs, keys=keys)\n",
    "#display(DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrlrrlllllllllllllrr}\n",
      "\\toprule\n",
      " & reads & bad  & uniq bad  & bad none & bad Q60 & uniq bad Q60 & bad INV & bad DUP & bad DEL & bad INS & bad multi & aligned_with 0/1/2+_segments & mean start diff & mean end diff & alignments depth & covered reference & index time & map time & memory & mult bad  & mult bad Q60 \\\\\n",
      "\\midrule\n",
      "sweepmap & 638 & 118 & 118 & 172 / 416 (41.35%) & 144 & 144 & 24 / 58 (41.38%) & 24 / 59 (40.68%) & 14 / 25 (56.00%) & 14 / 37 (37.84%) & 14 / 43 (32.56%) & 0 / 638 / 0 (0%/100%/0%) & -1082619.9 & -1082562.3 & 0.10 & 5762584 (9.23%) & 0.8 & 0.2 & 0.6 & NaN & NaN \\\\\n",
      "sweepmap-slow & 638 & 118 & 118 & 93 / 416 (22.36%) & 24 & 24 & 10 / 58 (17.24%) & 14 / 59 (23.73%) & 9 / 25 (36.00%) & 9 / 37 (24.32%) & 7 / 43 (16.28%) & 0 / 638 / 0 (0%/100%/0%) & -1045488.8 & -1045465.6 & 0.10 & 5981227 (9.58%) & 1.0 & 60.7 & 0.7 & NaN & NaN \\\\\n",
      "mapquik & 638 & 206 & 206 & 155 / 365 (42.47%) & 31 & 31 & 17 / 52 (32.69%) & 20 / 54 (37.04%) & 10 / 21 (47.62%) & 15 / 29 (51.72%) & 20 / 41 (48.78%) & 76 / 562 / 0 (12%/88%/0%) & -844564.3 & -844567.0 & 0.09 & 5210632 (8.34%) & 0.6 & -0.0 & 1.5 & NaN & NaN \\\\\n",
      "blend & 638 & 204 & 124 & 188 / 416 (45.19%) & 66 & 61 & 22 / 58 (37.93%) & 19 / 59 (32.20%) & 12 / 25 (48.00%) & 14 / 37 (37.84%) & 15 / 43 (34.88%) & 0 / 543 / 95 (0%/85%/15%) & -999652.6 & -1000080.2 & 0.12 & 5610945 (8.98%) & 1.1 & 3.9 & 0.2 & 80.000000 & 5.000000 \\\\\n",
      "minimap & 638 & 110 & 53 & 91 / 416 (21.88%) & 23 & 16 & 10 / 58 (17.24%) & 8 / 59 (13.56%) & 7 / 25 (28.00%) & 8 / 37 (21.62%) & 9 / 43 (20.93%) & 0 / 557 / 81 (0%/87%/13%) & -1044700.3 & -1044910.7 & 0.14 & 5763502 (9.23%) & 1.6 & 17.8 & 0.4 & 57.000000 & 7.000000 \\\\\n",
      "winnowmap & 638 & 89 & 52 & 83 / 416 (19.95%) & 41 & 25 & 9 / 58 (15.52%) & 10 / 59 (16.95%) & 9 / 25 (36.00%) & 9 / 37 (24.32%) & 10 / 43 (23.26%) & 0 / 572 / 66 (0%/90%/10%) & -1064044.2 & -1064158.5 & 0.12 & 5875957 (9.41%) & 2.7 & 689.2 & 0.5 & 37.000000 & 16.000000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(vcf_df.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
